from langchain.chat_models import ChatOpenAI
from langchain.embeddings import OpenAIEmbeddings
from langchain.vectorstores import FAISS
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.schema import Document, SystemMessage, HumanMessage
from dotenv import load_dotenv
import os
import sqlite3
from typing import List

from crud.chatbot import get_user_summary, save_user_summary, save_user_keywords, get_user_keywords
from models.user import User
from sqlalchemy.orm import Session

# ğŸ” í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ ë° LLM êµ¬ì„±
load_dotenv()
openai_api_key = os.getenv("OPENAI_API_KEY")
llm = ChatOpenAI(openai_api_key=openai_api_key, model_name="gpt-3.5-turbo", temperature=0)
embeddings = OpenAIEmbeddings(openai_api_key=openai_api_key)

# ğŸœ prac.db ì‹ë‹¹ ë°ì´í„° ë¡œë”©
conn = sqlite3.connect("prac.db")
cursor = conn.cursor()
cursor.execute("SELECT id, name, address, phone, rating, latitude, longitude FROM restaurants")
rows = cursor.fetchall()

documents = []
for row in rows:
    id_, name, address, phone, rating, lat, lng = row
    menu_text = "ë©”ë‰´ ì •ë³´ ì—†ìŒ"
    content = f"""ì‹ë‹¹ëª…: {name}
ì£¼ì†Œ: {address}
ì „í™”ë²ˆí˜¸: {phone}
í‰ì : {rating}
ë©”ë‰´:
{menu_text}
"""
    documents.append(Document(
        page_content=content,
        metadata={"id": id_, "latitude": lat, "longitude": lng, "rating": rating}
    ))

docs = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=100).split_documents(documents)
vectorstore = FAISS.from_documents(docs, embeddings)

# í‚¤ì›Œë“œ ì¶”ì¶œ
def extract_keywords_from_summary(summary: str) -> List[str]:
    messages = [
        SystemMessage(content="ë‹¤ìŒ ìš”ì•½ì—ì„œ ì‚¬ìš©ìì˜ ìŒì‹ ì·¨í–¥ì„ ë‚˜íƒ€ë‚´ëŠ” í•µì‹¬ í‚¤ì›Œë“œ 3~5ê°œë§Œ ì‰¼í‘œë¡œ ì¶”ì¶œí•´ì¤˜."),
        HumanMessage(content=summary)
    ]
    result = llm(messages).content.strip()
    return [kw.strip() for kw in result.split(",") if kw.strip()]

# ì˜ë„ ë¶„ë¥˜
def classify_intent(query: str, summary: str) -> str:
    messages = [
        SystemMessage(content="ë‹¤ìŒ ì§ˆë¬¸ì´ ìŒì‹ ì¶”ì²œ ì˜ë„ì¸ì§€ ëŒ€í™” ì˜ë„ì¸ì§€ íŒë‹¨í•´ì¤˜. ê²°ê³¼ëŠ” 'ì¶”ì²œ' ë˜ëŠ” 'ëŒ€í™”'ë¡œë§Œ ì‘ë‹µí•´."),
        HumanMessage(content=f"ìš”ì•½: {summary}\nì§ˆë¬¸: {query}")
    ]
    return llm(messages).content.strip()

# ìš”ì•½ ì¤„ ìˆ˜ ì œí•œ (10ì¤„ ì´ìƒ ì‹œ í•œ ì¤„ì”© ì‚­ì œ)
def truncate_if_too_long(text: str, max_lines: int = 10) -> str:
    lines = text.strip().split("\n")
    if len(lines) > max_lines:
        return "\n".join(lines[-max_lines:])
    return text

# ë©”ì¸ ì‘ë‹µ ìƒì„± í•¨ìˆ˜
def generate_chat_response(user: User, message: str, db: Session):
    # ì‚¬ìš©ì ì„ í˜¸ ì§ˆë¬¸ ê°ì§€
    if "ì¢‹ì•„í•˜ëŠ” ìŒì‹" in message or "ë‚´ê°€ ë­˜ ì¢‹ì•„" in message:
        keywords_obj = get_user_keywords(db, user.id)
        if keywords_obj and keywords_obj.keywords:
            return {"response": f"ë‹¹ì‹ ì€ ì´ëŸ° ìŒì‹ë“¤ì„ ì¢‹ì•„í•œë‹¤ê³  í•˜ì…¨ì–´ìš”: {keywords_obj.keywords}"}
        else:
            return {"response": "ì•„ì§ ë‹¹ì‹ ì˜ ìŒì‹ ì·¨í–¥ì„ ì˜ ëª°ë¼ìš”. ë§¤ìš´ ìŒì‹, í•œì‹ ë“± ë§ì”€í•´ ì£¼ì„¸ìš”!"}

    # summary ë¶ˆëŸ¬ì˜¤ê¸° + ê¸¸ì´ ì œí•œ
    summary_obj = get_user_summary(db, user.id)
    summary = getattr(summary_obj, "summary", "")
    summary = truncate_if_too_long(summary)

    # ì˜ë„ ë¶„ë¥˜
    intent = classify_intent(message, summary)

    if intent != "ì¶”ì²œ":
        messages = [
            SystemMessage(content="ë„ˆëŠ” ìŒì‹ ì¶”ì²œ ì±—ë´‡ì´ì•¼. ì‚¬ìš©ìì™€ ìì—°ìŠ¤ëŸ½ê²Œ ëŒ€í™”í•´."),
            HumanMessage(content=message)
        ]
        result = llm(messages).content.strip()

        summary += f"\n[User]: {message}\n[Bot]: {result}"
        summary = truncate_if_too_long(summary)
        save_user_summary(db, user.id, summary)

        return {"response": result}

    # ì¶”ì²œ ìš”ì²­ ì²˜ë¦¬ (ê¸°ë³¸ 2ê°œ ì¶”ì²œ)
    results = vectorstore.similarity_search(message, k=2)
    if not results:
        return {"response": "ì…ë ¥í•˜ì‹  ì •ë³´ë¡œëŠ” ì¶”ì²œí•  ìˆ˜ ìˆëŠ” ì‹ë‹¹ì´ ì—†ìŠµë‹ˆë‹¤. ìœ„ì¹˜ë‚˜ í‚¤ì›Œë“œë¥¼ ë‹¤ì‹œ ì‹œë„í•´ ì£¼ì„¸ìš”."}

    recommended = []
    for doc in results:
        lines = doc.page_content.strip().splitlines()
        name_line = next((line for line in lines if "ì‹ë‹¹ëª…:" in line), lines[0])
        recommended.append(f"ğŸ“ {name_line.strip()}")

    response_text = "\n".join(recommended)

    summary += f"\n[User]: {message}\n[Bot]: {response_text}"
    summary = truncate_if_too_long(summary)
    save_user_summary(db, user.id, summary)

    keywords = extract_keywords_from_summary(summary)
    save_user_keywords(db, user.id, keywords)

    return {"response": response_text}
